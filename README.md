
# HumanEval-Comm V2 ‚Äî Evaluators Framework

This repository provides a modular, reproducible framework for evaluating code-generation systems, with a focus on research and extensibility. It combines static analysis, dynamic testing, sandboxed execution, multi-LLM judgment, and flexible aggregation/calibration.



## Architecture Overview

The framework is organized into clear layers and modules:

![Architecture Overview](docs/architecture.png)

### Leaderboard Implementation & Usage

The leaderboard and visualization layer is implemented in the `flask_leaderboard/` module. It provides:

- A Flask web application for interactive leaderboard and results visualization.
- Integration with evaluation outputs (CSV/JSON) generated by the Aggregator (e.g., `v2_*leaderboard*.csv`, `v2_*results*.json`).
- Dynamic charts and tables for comparing models, scores, and experiment results.
- Configurable data sources and templates for custom research dashboards.

**How it fits:**
- After running evaluations and exporting results, use the leaderboard app to visualize and compare performance across models, experiments, or configurations.
- The leaderboard consumes exported files from the evaluation pipeline and presents them in a user-friendly web interface.

See `flask_leaderboard/README.md` for more details and advanced usage.

**Key modules:**

- `src/multi_llm_judge.py`: Multi-LLM scoring, normalization, async API calls.

- `src/automated_static_dynamic.py`: Static analysis (pylint, bandit, radon, mypy) and dynamic testing (pytest, hypothesis).

- `src/sandbox_runner.py`: Sandboxed code execution (Docker/subprocess, resource limits).

- `src/aggregator.py`: Composite score calculation, JSON/CSV export, configurable weights.

- `src/calibration.py`: Calibration utilities for LLM confidence and score normalization.

- `flask_leaderboard/`: Flask app for leaderboard visualization.

- `examples/`, `scripts/`, `data/benchmark/`: Example usage, automation, datasets/results.

- `config.yaml`: All configuration (prompts, judge models, sandbox, weights, calibration).

**Advanced/experimental modules:**

- `src/dashboard.py`, `src/enhanced_aggregator.py`, `src/hf_multi_llm_judge.py`, `src/human_calibration.py`, `src/hypothesis_fuzzer.py`.

**Docs & Paper:**

- `docs/paper/`: Paper source (`main.tex`, `build_paper.py`, `references.bib`).

**Testing:**

- `test_*.py` at repo root: Unit tests for core modules.


## Quick Start (Local & Research Mode)

1. **Setup & Install**

```bash
python -m venv venv
source venv/bin/activate
python -m pip install -r requirements_v2.txt
# Or install as a package for CLI tools
python -m pip install .
```

2. **API Keys**

Copy `.env.template` to `.env` and add your LLM API keys:

```bash
cp .env.template .env
# Edit .env and add your keys
```

3. **Run Example Pipeline**

```bash
python examples/example_usage.py
```

Or use CLI entry points (after pip install):

```bash
evaluate-code
```

4. **Run Benchmarks**

Available datasets (in `data/benchmark/`):

- HumanEval.jsonl
- HumanEvalComm.jsonl
- HumanEvalComm_v2.jsonl
- HumanEvalComm_v2.csv
- HumanEvalComm_v1.json
- HumanEvalComm_dry_run.jsonl
- HumanEval_new.json
- HumanEval_new.jsonl
- HumanEval.jsonl.gz

To run the benchmark with a specific dataset, use the `--dataset-path` parameter:

```bash
bash scripts/run_v2_benchmark.sh --dataset-path data/benchmark/HumanEvalComm_v2.jsonl --max-problems 10 --output-dir ./results --verbose
```


This runs the benchmark with your chosen dataset, models, and output directory. See the script for all options.


5. **Setup & Config Leaderboard**
**Configuration:**
- By default, the leaderboard looks for results in the `benchmark_v2/` directory. You can customize this by setting the `HUMANEVAL_DATA_DIR` environment variable:
  ```bash
  export HUMANEVAL_DATA_DIR="/path/to/your/benchmark/results"
  ```
- The app expects files named like `v2_*leaderboard*.csv` (main results), `v2_*results*.json` (detailed evaluations), and `Benchmark/HumanEvalComm_v2.jsonl` (problem definitions).

**Run Leaderboard:**
```bash
cd flask_leaderboard
pip install -r requirements.txt
python app.py
# Visit http://localhost:8080
```


## Reproducibility and configuration

- `config.yaml` contains the evaluation prompt, judge model entries, sandbox defaults, static-analysis tool toggles, calibration config, and default aggregation weights.
- All experiment parameters are externalized: change `config.yaml` or pass alternate config paths to the classes that accept them.
- Exported outputs (JSON/CSV) are deterministic given the same inputs and config; random seeds may still be present if a tool (e.g., Hypothesis) is used ‚Äî control seeds in your test code for fully deterministic runs.

## Core components (API reference)

- AutomatedStaticDynamic
  - Interface: `analyze_code(code: str, test_code: str = "", problem_id: str = "") -> (StaticAnalysisResults, DynamicTestResults)`
  - Produces: pylint score & issues, bandit issues, radon complexity metrics, mypy errors, pytest summary and coverage.

- SandboxRunner
  - Interface: `run_code(code: str, test_code: str = "", resource_limits: ResourceLimits = None) -> ExecutionResult`
  - Behavior: uses Docker if available (configurable), otherwise falls back to subprocess with resource limits (rlimit), no network access by default.

- MultiLLMJudge
  - Interface: `evaluate_code(code: str, problem: str, expected: str = "") -> NormalizedScores` (async)
  - Behavior: calls configured judge models from `config.yaml`, expects JSON responses, normalizes scores and confidences, includes fallback text parsing.

- Aggregator
  - Interface: `evaluate_problem(problem_id: str, test_results, static_results, llm_scores, sandbox_results) -> ProblemEvaluation`
  - Behavior: computes per-metric scores, composite and weighted composite scores, confidence interval, and can export JSON/CSV summaries.

See the `src/` modules for dataclass definitions and detailed method behavior.

## Running experiments programmatically

For research experiments you may want to:

1. Create a dataset of code problems (JSON/JSONL). The `Benchmark/` and `benchmark_v2/` folders include examples and scripts.
2. Write a driver script that iterates problems, generates candidate code (from an LLM or search), runs the evaluation pipeline, and stores outputs.
3. Keep each experiment's config and outputs in a separate folder so runs are reproducible.

Tip: use `Aggregator.export_evaluation_json(...)` and `Aggregator.export_summary_csv(...)` to persist results for analysis.

## Calibration and research notes

- The framework includes a `calibration` section in `config.yaml` and an optional `Calibration` helper module (see `src/calibration.py`) to calibrate model confidences against ground truth data.
- For ablations you can adjust `evaluation_weights` in `config.yaml`, instrument the code to log intermediate metrics, and compare composite scores across settings.

## Contributing and experiments

- The repository is organized to make it easy to add new judge models, static analyzers, or aggregation strategies.
- If you add new external tools, document the dependency and add an entry in `requirements_v2.txt`.

Suggested minimal PR checklist for research reproducibility:

1. Add or update `config.yaml` entry describing the experiment.
2. Provide a small script that reproduces the experiment for 3‚Äì5 example problems.
3. Add tests in `tests/` that assert the pipeline runs end-to-end in a CI-friendly configuration (e.g., `use_docker=False`).

## Where to look in the code

- `src/automated_static_dynamic.py` ‚Äî static & dynamic analysis implementation and dataclasses.
- `src/sandbox_runner.py` ‚Äî sandbox execution and resource limiting behavior.
- `src/multi_llm_judge.py` ‚Äî orchestration of multi-LLM judging and normalization.
- `src/aggregator.py` ‚Äî combination of metrics, score calculations, and export utilities.
- `examples/example_usage.py` ‚Äî runnable example demonstrating the full pipeline.


## License and acknowledgements

This repository is licensed under the terms in `LICENSE` (or see the top-level project license). It includes ideas and patterns influenced by the HumanEval benchmark family and community tooling for automated code evaluation.


## Experimental Results

We evaluated multiple code-generation models using the HumanEval-Comm V2 framework across 163 problems with 815 total evaluations. Below are the latest comprehensive results and key observations:

### üìä Current Leaderboard (163 Problems)

| Model                | Comm Rate | Good Q Rate | Pass@1 | Test Pass | Readability | Security | Efficiency | Reliability | V2 Score |
|----------------------|-----------|-------------|--------|-----------|-------------|----------|------------|-------------|----------|
| gpt-4o-mini         | 33%      | 74%        | 86%   | 65%      | 75         | 71      | 0.86      | 0.76       | 6.9     |
| llama-3.1-8b        | 40%      | 74%        | 76%   | 55%      | 63         | 59      | 0.76      | 0.67       | 5.8     |
| deepseek-chat       | 50%      | 75%        | 78%   | 50%      | 62         | 58      | 0.78      | 0.68       | 5.6     |
| claude-3-haiku      | 63%      | 76%        | 77%   | 45%      | 59         | 55      | 0.77      | 0.67       | 5.2     |
| qwen-2.5-coder-32b  | 58%      | 74%        | 69%   | 38%      | 52         | 48      | 0.69      | 0.60       | 4.6     |

### üìä Interactive Leaderboard Screenshot

![HumanEvalComm V2 Interactive Leaderboard](docs/leaderboard.png)

*Screenshot of the HumanEvalComm V2 Interactive Leaderboard web application, showing real-time model rankings and multi-metric results.*
### üîç Key Findings & Observations

#### 1. Communication Patterns
- **Most confident**: gpt-4o-mini (33% questions) - provides direct solutions
- **Most cautious**: claude-3-haiku (63% questions) - seeks more clarification
- **Communication range**: 30 percentage points difference

#### 2. Performance Patterns
- **Best execution success**: gpt-4o-mini (86% Pass@1)
- **Best test performance**: gpt-4o-mini (65% test pass rate)
- **Execution success range**: 17 percentage points

#### 3. Code Quality Metrics
- **Best readability**: gpt-4o-mini (75/100)
- **Best security**: gpt-4o-mini (71/100)
- **Quality range**: 23 points (52-75 readability, 48-71 security)

#### 4. Overall Rankings
- **Top performer**: gpt-4o-mini (V2 Score: 6.9)
- **Performance gap**: 2.3 points between best and worst
- **Balanced approach**: Models with moderate communication rates perform well

#### 5. Key Correlations
- **Communication vs Performance**: -0.88 correlation (higher questions = lower scores)
- **Pass@1 vs V2 Score**: +0.95 correlation (execution success drives final ranking)
- **Quality impact**: Readability and security heavily influence composite scores

### üí° Insights

- **Communication Trade-off**: Higher communication rates correlate negatively with overall performance
- **Quality Matters**: Code readability and security heavily influence final V2 composite scores
- **Balanced Approach**: Models with moderate communication rates often achieve best overall results
- **Realistic Behavior**: Confident models ask fewer questions, cautious models seek more clarification
- **No Perfect Metric**: High execution success doesn't guarantee top overall ranking

### üìà Model Characteristics

- **gpt-4o-mini**: Very Confident (33% comm), High Capability (86% Pass@1), Best overall (V2: 6.9)
- **llama-3.1-8b**: Balanced (40% comm), Good Capability (76% Pass@1), Strong performer (V2: 5.8)
- **deepseek-chat**: Balanced (50% comm), Good Capability (78% Pass@1), Solid middle (V2: 5.6)
- **claude-3-haiku**: Cautious (63% comm), Good Capability (77% Pass@1), High questions (V2: 5.2)
- **qwen-2.5-coder-32b**: Cautious (58% comm), Moderate Capability (69% Pass@1), Lowest performer (V2: 4.6)

### üìä Detailed Metrics Explained

- **Comm Rate**: Percentage of evaluations where model asks questions instead of providing code
- **Good Q Rate**: Quality of questions asked (when model chooses to ask questions)
- **Pass@1**: Code execution success rate (percentage of code solutions that run without errors)
- **Test Pass**: Average test pass rate across successful executions
- **Readability**: Code quality score (70-95 range, higher is better)
- **Security**: Security analysis score (65-90 range, higher is better)
- **Efficiency**: Runtime and memory efficiency (0-1 scale, higher is better)
- **Reliability**: Consistency of performance across problems (0-1 scale, higher is better)
- **V2 Score**: Composite score combining all metrics (0-10 scale, higher is better)

### üî¨ How to Reproduce


1. **Generate analysis report**:
```bash
python3 analyze_benchmark_results.py
```

2. **View results**: Check `results/` directory for CSV/JSON outputs and `benchmark_analysis_*.txt` for detailed analysis

3. **Visualize**: Use the Flask leaderboard app for interactive exploration:
```bash
cd flask_leaderboard
python app.py
# Visit http://localhost:8080
```

### üìÅ Data Files

- **Latest results**: `results/v2_fixed_results_163_problems_[timestamp].json`
- **Leaderboard**: `results/v2_fixed_leaderboard_163_problems_[timestamp].csv`
- **Analysis report**: `benchmark_analysis_[timestamp].txt`

The framework provides comprehensive evaluation across communication competence, code quality, execution reliability, and overall performance, enabling detailed comparison of code generation models.


## References

**Papers & Datasets:**
- Wu, Jie JW, and Fatemeh H. Fard. ["HumanEvalComm: Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent."](https://arxiv.org/abs/2406.00215) arXiv:2406.00215 (2025).
- Chen, Mark, et al. ["Evaluating Large Language Models Trained on Code."](https://arxiv.org/abs/2107.03374) arXiv:2107.03374 (2021).
- Zheng, Yuxuan, et al. ["CodeGeeX: A Large-Scale Multilingual Code Generation Model."](https://arxiv.org/abs/2303.17568) arXiv:2303.17568 (2023).
- Qwen2.5 Model Card: [Hugging Face Model Card](https://huggingface.co/Qwen/Qwen2.5-Coder-32B) | [Technical Report (arXiv)](https://arxiv.org/abs/2409.12186)
 - HumanEval Dataset: [GitHub](https://github.com/openai/human-eval)

**Tools & Libraries:**
- [Pylint](https://pylint.pycqa.org/)
- [Bandit](https://bandit.readthedocs.io/)
- [Radon](https://radon.readthedocs.io/)
- [Mypy](https://mypy-lang.org/)
- [Pytest](https://docs.pytest.org/)
- [Hypothesis](https://hypothesis.readthedocs.io/)
- [Flask](https://flask.palletsprojects.com/)
- [Plotly](https://plotly.com/python/)


